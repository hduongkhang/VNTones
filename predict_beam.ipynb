{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"predict_beam.ipynb","provenance":[{"file_id":"1Mm1MXfFQ-BFq4ku5HNi6Qu5IZRTVEwDT","timestamp":1567302992563}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gnXhFW6r8ug4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1595875680027,"user_tz":-420,"elapsed":24602,"user":{"displayName":"Khang Huỳnh Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLV01IZL2W7xkx0fkpZlJRdcPI3posR0tYoxJw=s64","userId":"01556900014716348589"}},"outputId":"0e51f759-ecf4-44a4-98e8-67cd60d72f10"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BRJ9yHJOrBOT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1595875692238,"user_tz":-420,"elapsed":9343,"user":{"displayName":"Khang Huỳnh Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLV01IZL2W7xkx0fkpZlJRdcPI3posR0tYoxJw=s64","userId":"01556900014716348589"}},"outputId":"1179646c-bbd3-4a02-c1d5-de0a602b3d5f"},"source":["import numpy as np\n","!pip install Unidecode\n","try:\n","    import re2 as re\n","except ImportError:\n","    import re\n","import numpy as np\n","from unidecode import unidecode\n","import os\n","from collections import Counter\n","from keras.models import load_model\n","from numpy import array\n","from numpy import argmax\n","from nltk import ngrams\n","from nltk.translate.bleu_score import sentence_bleu\n","import string\n","import math\n","\n","path_checkpoint = '/content/drive/My Drive/VNTones/model'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting Unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\r\u001b[K     |█▍                              | 10kB 29.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 3.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n","\u001b[?25hInstalling collected packages: Unidecode\n","Successfully installed Unidecode-1.1.1\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"jeRq7CPIrM49","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595875693510,"user_tz":-420,"elapsed":914,"user":{"displayName":"Khang Huỳnh Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLV01IZL2W7xkx0fkpZlJRdcPI3posR0tYoxJw=s64","userId":"01556900014716348589"}}},"source":["accented_chars_vietnamese = [\n","    'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ',\n","    'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ',\n","    'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ',\n","    'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự',\n","    'í', 'ì', 'ỉ', 'ĩ', 'ị',\n","    'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ',\n","    'đ',\n","]\n","accented_chars_vietnamese.extend([c.upper() for c in accented_chars_vietnamese])\n","alphabet = list(('\\x00 _' + string.ascii_letters + string.digits + ''.join(accented_chars_vietnamese)))\n","\n","\n","\n","s1 = u'ÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚÝàáâãèéêìíòóôõùúýĂăĐđĨĩŨũƠơƯưẠạẢảẤấẦầẨẩẪẫẬậẮắẰằẲẳẴẵẶặẸẹẺẻẼẽẾếỀềỂểỄễỆệỈỉỊịỌọỎỏỐốỒồỔổỖỗỘộỚớỜờỞởỠỡỢợỤụỦủỨứỪừỬửỮữỰựỲỳỴỵỶỷỸỹ'\n","s0 = u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYy'\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"9dl-nZUexw_N","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595875727136,"user_tz":-420,"elapsed":15407,"user":{"displayName":"Khang Huỳnh Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLV01IZL2W7xkx0fkpZlJRdcPI3posR0tYoxJw=s64","userId":"01556900014716348589"}}},"source":["if (os.path.exists(path_checkpoint)):\n","    listEpochs = [x for x in os.listdir(path_checkpoint) if x[:6] == 'model_' and x[-3:] == '.h5']\n","    lastEpoch = max([int(x[-6:-3]) for x in listEpochs])\n","    lastEpochFile = path_checkpoint + '/' + 'model_%03d.h5' % (lastEpoch)\n","lastEpochFile = '/content/drive/My Drive/VNTones/model/model_046.h5'\n","\n","model = load_model(lastEpochFile)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"8dQZFY8Xc-W-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"ok","timestamp":1595876028288,"user_tz":-420,"elapsed":2341,"user":{"displayName":"Khang Huỳnh Dương","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLV01IZL2W7xkx0fkpZlJRdcPI3posR0tYoxJw=s64","userId":"01556900014716348589"}},"outputId":"2b0b3805-d7f6-4b8e-d0ac-cac2c1c83411"},"source":["\n","NGRAM = 5\n","MAXLEN = 30\n","\n","def remove_accents(input_str):\n","\ts = ''\n","\tfor c in input_str:\n","\t\tif c in s1:\n","\t\t\ts += s0[s1.index(c)]\n","\t\telse:\n","\t\t\ts += c\n","\treturn s\n","\n","def accent_sentence(sentence):\n","  list_phrases = extract_phrases(sentence)\n","  output = ()\n","  for phrases in list_phrases:\n","    if len(phrases.split()) < 2 or not re.match(\"\\w[\\w ]+\", phrases):\n","      output += phrases\n","    else:\n","      outputs = add_accent(phrases)\n","      output = outputs\n","      # output += add_accent(phrases)\n","      if phrases[-1] == \" \":\n","        output += \" \"\n","\n","  return output\n","\n","def text_cleaner(text):\n","    text = re.sub(r\"'s\\b\",\"\",text)\n","    # remove punctuations\n","    # INTAB = \"ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđẠẢÃÀÁÂẬẦẤẨẪĂẮẰẶẲẴÓÒỌÕỎÔỘỔỖỒỐƠỜỚỢỞỠÉÈẺẸẼÊẾỀỆỂỄÚÙỤỦŨƯỰỮỬỪỨÍÌỊỈĨÝỲỶỴỸĐ\"\n","    text = re.sub(\"[^a-zA-Z0-9ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđẠẢÃÀÁÂẬẦẤẨẪĂẮẰẶẲẴÓÒỌÕỎÔỘỔỖỒỐƠỜỚỢỞỠÉÈẺẸẼÊẾỀỆỂỄÚÙỤỦŨƯỰỮỬỪỨÍÌỊỈĨÝỲỶỴỸĐ ]\", \"\", text)\n","    return text\n","\n","def encode(text, maxlen=MAXLEN):\n","        text = \"\\x00\" + text\n","        x = np.zeros((maxlen, len(alphabet)))\n","        for i, c in enumerate(text[:maxlen]):\n","            x[i, alphabet.index(c)] = 1\n","        if i < maxlen - 1:\n","          for j in range(i+1, maxlen):\n","            x[j, 0] = 1\n","        return x\n","\n","def gen_list(ngrams_list):\n","    len_ngrams_list = len(ngrams_list)\n","    number_candidate = len(ngrams_list[0])\n","    index_arr = [[] for i in range(len_ngrams_list)]\n","    prob_arr = [[] for i in range(len_ngrams_list)]\n","    \n","    for i in range(len_ngrams_list):\n","        for j in range(number_candidate):\n","            index_arr[i].append(ngrams_list[i][j][0])\n","            prob_arr[i].append(ngrams_list[i][j][1])\n","    # print(index_arr)\n","    # print(prob_arr)\n","    return index_arr, prob_arr\n","\n","def extract_phrases(text):\n","    pattern = r'\\w[\\w ]*|\\s\\W+|\\W+'\n","    return re.findall(pattern, text)\n","  \n","def gen_ngrams(words, n=5):\n","    return ngrams(words.split(), n)\n","\n","def decode(x, calc_argmax=True):\n","  if calc_argmax:\n","      x = x.argmax(axis=-1)\n","  return ''.join(alphabet[i] for i in x)\n","  \n","def beam_search_decoder(data, k):\n","\tsequences = [[list(), 0.0]]\n","\t# walk over each step in sequence\n","\tfor row in data:\n","\t\tall_candidates = list()\n","\t\t# expand each current candidate\n","\t\tfor i in range(len(sequences)):\n","\t\t\tseq, score = sequences[i]\n","\t\t\tfor j in range(len(row)):\n","# \t\t\t\tprint(j)\n","# \t\t\t\tprint(row[j] - 1e-30 < 0)\n","\t\t\t\tcandidate = [seq + [j], score - math.log(row[j])]\n","\t\t\t\tall_candidates.append(candidate)\n","\t\t# order all candidates by score\n","\t\tordered = sorted(all_candidates, key=lambda tup:tup[1])\n","\t\t# select k best\n","\t\tsequences = ordered[:k]\n","\treturn sequences\n","\n","# beam search\n","def beam_search_decoder2(data, k):\n","\tsequences = [[list(), 0.0]]\n","\t# walk over each step in sequence\n","\tfor row in data:\n","\t\tall_candidates = list()\n","\t\t# expand each current candidate\n","\t\tfor i in range(len(sequences)):\n","\t\t\tseq, score = sequences[i]\n","\t\t\tfor j in range(len(row)):\n","\t\t\t\tcandidate = [seq + [j], score - math.log(row[j])]\n","\t\t\t\tall_candidates.append(candidate)\n","\t\t# order all candidates by score\n","\t\tordered = sorted(all_candidates, key=lambda tup:tup[1])\n","\t\t# select k best\n","\t\tsequences = ordered[:k]\n","\treturn sequences\n","  \n","def decode_bs(data):\n","  res = beam_search_decoder(data, 3)\n","  for i, seq in enumerate(res):\n","      res[i][0] = ''.join(alphabet[k] for k in seq[0]).strip('\\x00')\n","  return res\n","\n","\n","\n","def guess(ngram):\n","    text = ' '.join(ngram)\n","    # print(text)\n","    preds = model.predict(np.array([encode(text)]), verbose=0)\n","    # print(preds[0])\n","    return decode_bs(preds[0])\n","    # return decode(preds[0], calc_argmax=True)[1:len(text)+1], preds[0]\n","\n","\n","def candidates(guessed_ngrams):\n","  candidates = [Counter() for _ in range(len(guessed_ngrams) + NGRAM - 1)]\n","  for nid, ngram in enumerate(guessed_ngrams):\n","        for wid, word in enumerate(re.split(' +', ngram)):\n","            candidates[nid + wid].update([word])\n","  output = ' '.join(c.most_common(1)[0][0] for c in candidates)\n","  return output\n","\n","def Mapping(a,b):\n","  for i in range(len(b)):\n","    for j in range(len(b[0])):\n","      b[i][j] = b[i][j]\n","\n","  for i in range(len(b)):\n","    for j in range(len(a)):\n","      b[i][j] = a[j][b[i][j]]\n","  return b\n","\n","def add_accent(text):\n","    ngrams = list(gen_ngrams(text, n=NGRAM))\n","    guessed_ngrams = []\n","    candidates_index =[]\n","    for ngram in ngrams:\n","      ngram_set = guess(ngram)\n","      print(ngram_set)\n","      guessed_ngrams.append(ngram_set)\n","    index_arr, prob_arr = gen_list(guessed_ngrams)\n","\n","    for i in range(len(prob_arr)):\n","      for j in range(len(prob_arr[0])):\n","        prob_arr[i][j] = (1/prob_arr[i][j])\n","\n","    # print(prob_arr)\n","\n","    index_cand = beam_search_decoder2(prob_arr, 3)\n","\n","    for i in range(len(index_cand)):\n","      candidates_index.append(index_cand[i][0])\n","\n","    # print(candidates_index)\n"," \n","    candidates_index = Mapping(index_arr,candidates_index)\n","    a = [] \n","    # print(candidates_index)\n","    for candidates_sentence in candidates_index:\n","      a.append(candidates(candidates_sentence))\n","    return (a)\n","\n","text_1 = 'Điều này cũng hoàn toàn phù hợp với xu hướng phát triển của thị trường giai đoạn 3 năm qua'\n","text_1 = text_cleaner(text_1)\n","\n","text_kodau = remove_accents(text_1)\n","print(text_kodau)\n","ref = [text_1.split()]\n","# print(\"result from \"+lastEpochFile)\n","for i,result in enumerate(accent_sentence(text_kodau)[:3]):\n","  can = result.split()\n","  score = sentence_bleu(ref, can, weights=(1,0,0,0))\n","  print('kết quả dự đoán',i+1,': ',result,'===== Điểm BLEU:',score)\n","\n"," \n","    \n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Dieu nay cung hoan toan phu hop voi xu huong phat trien cua thi truong giai doan 3 nam qua\n","[['Điều này cũng hoàn toàn', 0.016023754686108244], ['Điều này cùng hoàn toàn', 4.407390822956666], ['Điều nay cũng hoàn toàn', 6.613001127366269]]\n","[['này cũng hoàn toàn phụ', 0.5692254446064041], ['này cũng hoàn toàn phù', 1.248585873392355], ['này cũng hoàn toàn phủ', 2.509784856093999]]\n","[['cũng hoàn toàn phù hợp', 0.20133753756643924], ['cùng hoàn toàn phù hợp', 2.2028823981543098], ['cứng hoàn toàn phù hợp', 3.220324368636257]]\n","[['hoàn toàn phù hợp với', 0.0017168911433487928], ['hoàn toàn phụ hợp với', 7.72777006201997], ['hoàn toàn phủ hợp với', 8.01064329987076]]\n","[['toán phù hợp với xu', 1.099002481356898], ['toàn phù hợp với xu', 1.6795115931838915], ['toán phù hợp với xử', 1.8367820185748314]]\n","[['phù hợp với xu hướng', 0.0032918195285906527], ['phủ hợp với xu hướng', 6.342000680611386], ['phụ hợp với xu hướng', 7.866159130959694]]\n","[['hợp với xu hướng phát', 0.00797897041945364], ['hợp với xu hướng phạt', 5.858358937295632], ['họp với xu hướng phát', 6.042327494825868]]\n","[['với xu hướng phát triển', 0.002703479542779893], ['với xu hướng phat triển', 6.309295454231701], ['với xu hướng phát triền', 8.761481162129188]]\n","[['xu hướng phát triển của', 0.004705062509461453], ['xu hướng phat triển của', 6.344497264224092], ['xử hướng phát triển của', 7.203722611719139]]\n","[['hướng phát triển của thị', 0.29941960897672254], ['hưởng phát triển của thị', 2.39279579742276], ['hướng phát triển của thí', 3.1045877754539273]]\n","[['phát triển của thị trường', 0.01324118944589373], ['phát triển của thị trưởng', 4.44856868060791], ['phat triển của thị trường', 7.110092546799329]]\n","[['triển của thị trường giải', 0.18130934413294245], ['triển của thị trường giai', 1.8655173464250052], ['triển của thị trưởng giải', 5.274409029360695]]\n","[['của thị trường giai đoạn', 0.04483069810881255], ['của thị trưởng giai đoạn', 3.367826871575692], ['của thị trường giai đoán', 5.365288163931494]]\n","[['thị trường giai đoạn 3', 0.02853278534045016], ['thị trưởng giai đoạn 3', 3.7857769067417184], ['thị trường giai đoán 3', 6.381060310921301]]\n","[['trưởng giai đoạn 3 năm', 0.49593096975262196], ['trường giai đoạn 3 năm', 1.2784875317073907], ['trương giai đoạn 3 năm', 3.020970773697102]]\n","[['giai đoạn 3 năm qua', 0.14782928233033873], ['giai đoạn 3 năm quá', 3.046101821139385], ['giai đoạn 3 nằm qua', 3.145771221911428]]\n","kết quả dự đoán 1 :  Điều này cũng hoàn toàn phù hợp với xu hướng phát triển của thị trường giai đoạn 3 năm qua ===== Điểm BLEU: 1.0\n","kết quả dự đoán 2 :  Điều này cũng hoàn toàn phù hợp với xu hướng phát triển của thị trường giai đoạn 3 năm qua ===== Điểm BLEU: 1.0\n","kết quả dự đoán 3 :  Điều này cũng hoàn toàn phù hợp với xu hướng phát triển của thị trường giai đoạn 3 năm qua ===== Điểm BLEU: 1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bWY7Xa56wSCg","colab_type":"code","colab":{}},"source":["!ls -la"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J6YMj0gOkEdt","colab_type":"text"},"source":[""]}]}